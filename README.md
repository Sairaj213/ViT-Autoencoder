# ðŸ“· Transformer Autoencoder for Image Reconstruction
<br>

This is an Autoencoder architecture built with Transformer and Keras, for image reconstruction on the CIFAR-10 dataset. The architecture uses a Vision Transformer (ViT) style of patching images, encoding them with Transformer blocks, and then decoding them back into images.
<br>
## Project Structure

```
â”œâ”€â”€ run.py                  # Main entry point to run the project
â”œâ”€â”€ config.py               # All hyperparameters and file paths
â”œâ”€â”€ data_utils.py           # Data loading and preprocessing for CIFAR-10
â”œâ”€â”€ model.py                # Contains the Keras model architecture and custom layers
â”œâ”€â”€ train.py                # Handles the model training loop and saving logic
â”œâ”€â”€ evaluation.py           # Functions for visualizing results and calculating metrics
â”œâ”€â”€ requirements.txt        # Project dependencies
â””â”€â”€ README.md               # This file
```

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/Sairaj213/ViT-Autoencoder.git
    cd transformer_autoencoder
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
    ```

3.  **Install the dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    If you have a GPU, ensure you have the correct version of CUDA and cuDNN installed and use `tensorflow[and-cuda]` or `tensorflow-gpu` if needed.

## How to Run

To run the entire pipelineâ€”including data loading, training, model saving, and evaluationâ€”simply execute the `run.py` script:

```bash
python run.py
```

The script will:
1.  Load and prepare the CIFAR-10 dataset.
2.  Build and compile the Transformer Autoencoder model.
3.  Train the model for the number of epochs specified in `config.py`.
4.  Save the trained model to `transformer_autoencoder.keras`.
5.  Generate and save a plot of the training history (`training_history.png`).
6.  Load the saved model and generate reconstructions of test set images (`test_reconstructions.png`).
7.  Evaluate a single sample image (`sample_image.jpg`) and print its quality metrics (MSE, PSNR, SSIM).

## Configuration

All key parameters can be modified in `config.py`. This includes:
- **Model architecture:** `IMAGE_SIZE`, `PATCH_SIZE`, `PROJECTION_DIM`, etc.
- **Training settings:** `EPOCHS`, `BATCH_SIZE`, `LEARNING_RATE`.
- **File paths:** Location for saving the model and output plots.

To test with your own image, replace `sample_image.jpg` with your image and update the `SAMPLE_IMAGE_PATH` in `config.py`.


After running `run.py`, the following files will be generated in your project directory:
- `transformer_autoencoder.keras`: The saved, trained Keras model.
- `training_history.png`: A plot of the training and validation loss over epochs.
- `test_reconstructions.png`: A comparison of original and reconstructed images from the test set.
